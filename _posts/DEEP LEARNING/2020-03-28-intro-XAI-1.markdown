---
layout: post
title:  "Introduction to Explainable AI (1)"
subtitle:   "Introduction to Explainable AI"
categories: data
tags: dl
comments: true
use_math: true
---

**<font color = "#aba9a6">이번 글은 내용이 조금 길어져서 두 번으로 나누어서 포스팅하도록 하겠습니다.</font>**😅 <br>

> "Interpretability is the degree to which a human can understand the cause of a decision." (Miller 2017)

* 최근 [XAI 설명 가능한 인공지능, 인공지능을 해부하다](https://wikibook.co.kr/xai/) 책이 출간되어 주문한 기념으로, Explainable AI(설명 가능한 인공지능)를 소개하는 글을 쓰게 되었습니다.
* [Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models](https://arxiv.org/abs/1708.08296) 논문을 정리하고 조금의 추가적인 내용을 덧붙인 글입니다.

-------

인공지능(Artificial Intelligence, AI) 기술은 사용자에게 추천과 예측 등의 정보를 제공하며, 거의 모든 분야에 다양한 용도로 사용되고 있습니다. 하지만 알고리즘의 복잡성으로 인해 어떻게 이런 결정을 했는지 파악할 수 없기에, 이를 AI의 '블랙박스'라고 부릅니다. 도출한 최종 결과의 근거와 도출 과정의 타당성을 제공하지 못하는 이슈가 존재하고, 이를 해결하고자 등장한 것이 “**설명가능한 AI(Explainable AI, XAI)**”입니다. <br>


## 1. Introduction
본 논문의 경우 2017년에 쓰여진 논문이다보니, 2016년에 있었던 이세돌 선수와 알파고의 대결을 예시로 들고 있습니다.
![이세돌](https://user-images.githubusercontent.com/54492747/77823456-aba74a80-713e-11ea-9f34-af3fc40e4c3a.jpg)
이세돌 선수와의 대결에서, 알파고가 '왜 해당 지점에 바둑돌을 두었을까'에 대한 의문에 대해 전혀 투명하게 정보가 제공되지 않았다는 점에서 문제가 제기됩니다. 알파고 접근법 성공에 대한 논리적 근거 부족성으로 인해, 알파고의 결정 과정을 이해하거나 검증할 수 없기 때문입니다.

입력 데이터(input data)에 대해 어떤 정보가 모델의 결과에 도달했는지 명확하지 않은 모델을 '블랙박스(black-box) 모델'라고 합니다. 블랙박스라고 하는 이유는 데이터를 사용해 특정 결론에 이르는 과정이 투명하지 않고, 이와 같은 모호성으로 인해 시스템의 특정 행동 방식이나 그 이유를 알기가 어렵다는 데 있습니다. 그렇기 때문에 많은 분야에서 AI시스템의 의사결정 과정을 이해하고 검증하는 것이 불가능하다는 것은 큰 단점입니다.
1. 의료 <br>
생명이 걸린 문제이기 때문에  블랙박스 모델을 전적으로 신뢰하는 것은 굉장히 위험하고 무책임한 일입니다. 그래서 무엇보다 전문가가 적절하게 개입하고 확인이 필요한 영역입니다.
2. 자율주행 <br>
한번의 잘못된 예측이 매우 많은 비용을 초래하기 때문에 올바른 특징에 기반한 모델이 보장되어야 합니다.
3. 금융 <br>
고객의 개인적인 정보를 다루기 때문에 신뢰성과 공정성을 보장할 수 있어야 합니다. <br>

딥러닝 모델의 경우 우수한 성능을 보이더라도, 모델의 복잡한 비선형 구조로 인해 데이터 차원에서 그 요인을 파악하는 것은 어렵습니다. 즉, 비선형적인 구조가 지속적으로 나옴에 따라 정확한 예측에 도달하는 것에 대한 정보가 되지 않음으로 투명성 부족에 대한 문제가 야기됩니다. 이러한 문제를 해결하기 위해 딥러닝 모델이 판단한 근거와 판단 과정의 타당성에 대한 설명을 하고자 등장한 것이 바로 '설명가능한 AI(XAI)'입니다.

<br> 

### 🔮 설명가능한 AI
* 사용자가 인공지능 시스템의 동작과 최종 결과를 이해하고 올바르게 해석하여 결과물이 생성되는 과정을 설명 가능하도록 해주는 기술
* 모델이 내부적으로 작동하는 방식을 이해할 수 있게 해줌

🌱 예시 : AI 시스템이 고양이 이미지를 분류할 경우, 기존 시스템은 입력된 이미지의 고양이 여부만을 도출하지만 XAI는 고양이 여부를 도출하고 털과 콧수염, 발톱 등으로 고양이로 판단한 근거까지 사용자에게 제공함
![xai](https://user-images.githubusercontent.com/54492747/77825570-72c2a200-714d-11ea-9cb7-653a69acf785.PNG)
(사진 출처: [DARPA, Explainable Artificial Intelligence(XAI)](https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf))  

<br>
그리하여 머신러닝(ML)과 인공지능(AI)에 대한 설명성의 필요성에 대한 인식을 높이는 것을 본 논문의 주요 목표로 삼고 있습니다. 
 
<br>
 
## 2. Why do we need Explainable AI?

AI의 설명가능성을 찬성하는 총 4가지의 주장이 있으며 다음과 같습니다.
* 시스템 검증
* 시스템 개선
* 시스템으로부터의 학습
* 법규 준수

<br>

**1. 시스템 검증 (Verification of the system)** <br>
기본적으로 블랙박스 시스템을 전적으로 신뢰하면 안됩니다. 예를 들어, 의료 분야에서는 의료 전문가들에 의해 해석되고 검증될 수 있는 모델을 사용하는 것이 절대적으로 필요합니다. <br>
🌱 예시 : 사람의 폐렴 위험을 예측하도록 훈련된 AI 시스템 ([논문](https://www.aminer.cn/pub/5736973b6e3b12023e62b254/intelligible-models-for-healthcare-predicting-pneumonia-risk-and-hospital-day-readmission))
* 사전정보) 천식 또는 심장병 환자들은 전문가에 의해 엄격한 관리를 받고 환자들의 민감성이 증가함에 따라 사망 확률이 낮습니다.
* 위의 사전 정보를 모르고 AI 시스템은 건강한 사람과 대조적으로 체계적으로 편향된 데이터(biased data)를 학습하게 되며, 사망확률이 건강한 사람이 천식 환자보다 더 높다는 잘못된 결론에 도달하게 됩니다.
![부작용](https://user-images.githubusercontent.com/54492747/77824131-98e34480-7143-11ea-8a38-3df91342c838.PNG)

<br>
**2. 시스템 개선 (Improvement of the system)** <br>
AI 시스템 개선의 가장 첫번째 단계는 취약점을 이해하는 것입니다. 즉, AI 시스템에 있어서 어디에 결함이 있는지 분석하는 것이 중요합니다. 만약 모델이 수행하는 작업과 예측에 도달하는 이유를 이해한다면, 편향된 데이터를 덜어내는 것이 더 용이해지고 모델을 개선하기 더 쉬워질 것입니다.

<br>
**3. 시스템으로부터의 학습 (Learning from the system)** <br>
AI 시스템은 수백만개의 예시로 훈련되며, 이를 통해 사람이 접근할 수 없는 데이터에서 패턴을 관찰할 수 있습니다. XAI를 사용하게 된다면, 새로운 관점을 얻기 위해 AI 시스템에서 지식을 추출할 수 있을 것입니다.
과학의 경우, 과학자들은 블랙박스 모델로 어떠한 양을 예측하는 것보다 자연의 숨겨진 법칙을 식별할 수 있다는 것에 관심이 있습니다. 따라서 XAI는 이러한 과학의 영역에서 더욱 유용하게 사용될 것입니다.

<br>
**4. 법규 준수 (Compliance of legislation)** <br>
AI 시스템은 법적 질문과 그 결정의 이유에 대한 만족스러운 답변을 가져야 합니다. 투명성 있는 답변을 하기 위해 XAI의 필요성이 증가하게 됩니다. 예를 들어 은행에서 대출을 한다고 할 경우, 개인의 권리 측면에서도 은행의 대출 거부자에게 어떠한 이유로 이러한 결정을 하였는지 설명하는 것이 가능해야하는 필요성이 강조됩니다.
현재 유럽연합(EU)은 '설명권(right to explanation)'에 대한 새로운 규정을 적용함으로써, 사용자는 자신에 대한 알고리즘적 결정에 대한 설명을 요청할 수 있게 되었습니다.
 
<br>

## 3. Methods for Visualizing, Interpreting and Explaining Deep learning models
XAI를 만드는 방법은 아래와 같습니다.
* Sensitivity Analysis(SA)
* Layer-Wise Relevance Propagation(LRP)
* LIME
* Decision trees
* Deep Taylor Decomposition
* DeepLift
* KT-method
* Validity Internal analysis

<br>

본 논문에서는 딥러닝 모델의 설명 및 예측을 위한 대표적인 2가지 해석 기법(SA, LRP)을 소개합니다. AI 시스템의 예측을 설명하는 프로세스는 아래의 그림과 같습니다.
* 먼저 AI 시스템은 입력 이미지를 '수탉'으로 올바르게 분류합니다.
* 그런 다음 입력 변수 관점에서 해당 결정에 도달한 이유를 설명하기 위해 SA 또는 LRP와 같은 방법을 적용합니다.
* 이 설명의 결과는 예측에 대한 각 픽셀의 중요성을 시각화한 히트맵입니다. 히트맵을 사용하면 AI 시스템이 의도한대로 작동하는지 확인할 수 있습니다.
	* 이 예에서는 수탉의 붉은 볏과 고기수염이 AI 시스템의 예측 결정을 위한 부분에 해당합니다.
![논문내용](https://user-images.githubusercontent.com/54492747/77824839-d4ccd880-7148-11ea-83a7-3de4f4a3aca8.PNG)

<br>

### 1. Sensitivity Analysis(SA)
* 특징 : how much do changes in each pixel affect the prediction <br>

딥러닝 모델에서 국소적(local)인 입력 변화에 대한 예측 결과의 변화량을 정량화하여 입력 이미지의 어떤 부분이 딥러닝 모델의 결과 도출에 큰 영향을 미쳤는지 설명하는 방법입니다.

SA는 각 입력변수 i (ex.이미지 픽셀)의 중요도를 다음과 같이 편미분의 norm으로 계산합니다. <br>
![SA](https://user-images.githubusercontent.com/54492747/77831798-a36a0200-7174-11ea-8cda-af3524a7d98a.PNG)

이때 가장 관련성이 높은 입력 특징(input feature)을 가장 민감한 특징이라 가정하고, SA는 함수값 f(x) 자체를 설명하는 것이 아니라 함수 값의 변동(이미지의 예측값을 높이거나 낮추는 것은 무엇인지)을 설명합니다. <br>
SA로 계산된 히트맵은 이미지를 예측된 클래스와 비슷하거나 덜 비슷하게 보이게 하기 위해 어떤 픽셀을 변경해야 하는지를 나타냅니다.

<br>

![수탉](https://user-images.githubusercontent.com/54492747/77829275-63028800-7164-11ea-9be5-1306c99a01e3.PNG)
위의 그림을 예시로 들면, 변경해야하는 픽셀들은 수탉의 일부를 가리고 있는 노란꽃 부분입니다. 해당 픽셀을 특정 방법으로 바꾸면 수탉의 가려진 부분을 재구성할 수 있으며, 이를 통해 수탉이 더 많이 보일 것이기 때문에 분류 평가점수(classification score)를 높일 수 있습니다. <br>
하지만 SA의 히트맵은 수탉 예측에 실제로 어떤 픽셀이 중요한지 나타내지 않는 단점이 있기 때문에, SA는 AI 시스템의 예측을 설명하는 데 있어 최적의 방법이 될 수 없습니다.

<br>

### 2. Layer-Wise Relevance Propagation(LRP)
※ [파이콘 한국 2019 튜토리얼 - LRP (Part 2)](https://www.slideshare.net/OpenXAI/2019-lrp-part-2)에 자세히 설명되어 있습니다.

1) Layer-Wise : 레이어 단위로 <br>
2) Relevance : 결과에 영향을 주는 관련성을 구하는 <br>
3) Propagation : 역전파 기술 <br>
* 특징 : how much does each pixel contribute to prediction <br>

딥러닝 모델에서 예측 결과로부터 역전파 형태로 신경망의 각 레이어별 기여도를 측정할 수 있는 방법입니다. 이러한 방식은 딥러닝 모델의 부분 모듈인 각 레이어의 기여도를 히트맵 형태로 시각화하여 직관적으로 이해할 수 있습니다. 

![relevance](https://user-images.githubusercontent.com/54492747/77909281-0c15c380-72c8-11ea-8da0-12764c6134e0.png)
(출처 : [Explaining nonlinear classification decisions with deep Taylor decomposition](https://www.sciencedirect.com/science/article/pii/S0031320316303582))

<br>
AI 시스템의 예측을 분해하기 위해  feed-forward neural networks, long-short term memory의 프레임워크를 활용합니다. SA와 달리 LRP는 최대 불확실성(maximum uncertainty) 상태에 대한 예측을 설명하며, 수탉이라고 예측하는 데 있어 중요한 픽셀이 무엇인지 찾아냅니다. <br>
📢 SA가 편미분을 이용해서 히트맵을 추출하고 이미지를 인식한다면, LRP는 분해로 분류기의 결정을 설명하는 방식입니다. <br>

LRP는 local redistribution rule을 이용하여 각 입력변수(ex. 이미지 픽셀)에 관련성 점수(relevance score) $R_i$를 할당할 때까지 계속해서 거꾸로 재분배합니다. 재분배 과정의 주요 특징은 관련성 보존이며, 딥러닝의 모든 레이어에서 총 관련성인 예측 f(x)가 보존됩니다. <br>
![관련성 보존](https://user-images.githubusercontent.com/54492747/77831813-bbda1c80-7174-11ea-924f-f1b834f24d95.PNG)
재분배 중에 인위적으로 추가하거나 제거하지 않으며, 각 입력변수의 관련성 점수는 변수가 얼마나 예측에 기여했는지를 설명합니다. <br>
🌱 예시 : 이미지의 경우 각 이미지 픽셀이 이미지를 인식하는데 얼마나 기여했는지 설명하는 방식입니다.

<br>

다음은 feed-forward neural networks에 대한 LRP 재분배 과정에 대한 설명입니다. backward로 레이어 k=l+1에서 레이어 l로 관련성을 재분배하는 방식이며, 뉴런의 활성화 함수 $x_j$와 연결의 강도를 나타내는 가중치 $w_{jk}$에 따라 재분배됩니다. <br>
![feed](https://user-images.githubusercontent.com/54492747/77831825-cf858300-7174-11ea-8b58-9fb34a0b3827.PNG)
* $x_j$ : 레이어 l에서의 뉴런 활성화 함수
* $R_k$ : 레이어 l+1에서 뉴런과의 관련성 점수
* $w_{jk}$ : 뉴런 j와 뉴런 k를 연결하는 가중치
* $\epsilon$ : 0으로 나눠지는 것을 막는 역할 (하지만 관련성 보존으로 인해 0을 유지)

이때 $x_j$는 더 많이 활성화된 뉴런이 더 큰 관련성 평가점수를 부여받게 되고, $w_{jk}$는 더욱 두드러진 연결을 발생할수록 더 많은 관련성이 흐르게 됩니다.

<br>

관련성 보존은 제약조건인 $\alpha - \beta = 1$이라는 공식에 의해 시행되며, $\alpha = 1$인 경우에는 재분배 규칙 신경망은 ReLU 뉴런으로 구성되게 됩니다.
![알파베타룰](https://user-images.githubusercontent.com/54492747/77831522-0a86b700-7173-11ea-9466-66f85e908de2.PNG)

<br>

-----
분량 조절 실패로, 다음 포스팅에서 평가 방법과 실제 사례 부분에 대해 쓰도록 하겠습니다😂 논문의 양은 많지 않은데 추가적으로 내용을 포함시키니 시간이 많이 걸리네요!
