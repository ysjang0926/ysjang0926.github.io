---
layout: post
title:  "🚫멍청방지 개념 정리글🚫"
subtitle:   "Let's not be a fool"
categories: etc
tags: etc_til
comments: true
---

- 최근 논문을 읽으려고 하는데, 오랜만의 공부라 그런지 어려워하는 저를 보며 충격을 먹었습니다.
- 기본 개념조차 까먹고 있어서, 다시 공부 시작하는 겸 정리해보려고 합니다.
- 이번 글은 제가 다시 보기 위해 쓰는 글이다보니 다소 러프할 수 있습니다😢

---------

# 💡 통계분석 개념
### ⬛ 가설검정
1. **단측 검정(one-sided test)**
  * 표본 분포의 한쪽에 관심을 가지고 시행하는 검정방법
  * 즉, 대립가설이 어느 특정 모수 이상이거나 이하일 때 검정하는 것
2. **양측 검정(two-sided test)**
  * 차이가 있다, 라는 가설에 대한 검정
3. **유의수준(significant level)**
  * 가설 예측을 100% 옳게 할 수 없으므로 오차를 고려하기 위한 것
  * **P(TypeI Error) = 제 1종 오류가 발생할 확률*
  * 잘못해서 귀무가설을 기각하는 제1종 오류를 범할 확률의 상한
    * 1종 오류의 상한선
    * 검정결과 p-value가 유의수준보다 낮으면 상한선을 벗어나지 않으므로 귀무가설을 기각하고, p-value가 높으면 귀무가설을 채택 
  * 유의수준을 통해 귀무가설 채택여부를 결정 (일반적으로 0.05로 설정)
  * 검정 통계량 기반으로 p-value가 산출되면, 0.05보다 낮으면 귀무가설을 기각하고 대립가설을 채택함
4. **검정 통계량**
  * 귀무가설이 참이라는 가정 하에, 확률표본을 이용하여 구한 모수에 대한 추정량
  * 모수 추정을 위해 구하는 표본 통계량과 같은 의미
  * 표본을 통해 가설 검정에 사용하는 확률 변수
5. **1종 오류/2종 오류**
  * 1종 오류(type 1 error, alpha) : 귀무가설이 참인데 기각한 경우
    * p-value = 1종 오류를 얼마나 범할 확률
    * 즉, p-value가 5%라면 100번 검정하면 5번 정도 1종 오류가 발생하는 것
    * 유의수준 = 1종 오류의 상한선
  * 2종 오류(type 2 error, beta) : 대립가설이 사실임에도 불구하고 귀무가설을 기각하지 못하는 경우
  ![image](https://github.com/ysjang0926/ysjang0926.github.io/assets/54492747/909a88a3-b756-4969-b5c0-098aae02eced)

6. **검정력(statistical power)**
  * 1- beta
  * 대립가설이 사실일 때 귀무가설을 기각할 확률(1-beta) = 대립가설을 채택할 확률
  * **귀무가설이 사실이 아닐 때 이를 기각하여 올바른 결정을 할 수 있는 확률**
  * alpha를 고정시키고, 이를 만족시키는 기각역 중에 beta를 최소화하는 기각역을 선택

![image](https://github.com/ysjang0926/ysjang0926.github.io/assets/54492747/b44c7918-07e5-4d1e-a45e-dde5bc5d15bf)


### ⬛ 중심극한정리
무작위로 추출된 표본의 크기가 커질수록 표본 평균의 분포는 모집단의 분포 모양과는 관계없이 정규분포에 가까워진다는 정리
* 표본 평균의 평균은 모집단의 모평균과 같고, 표본 평균의 표준 편차는 모집단의 모표준 편차를 표본 크기의 제곱근으로 나눈 것과 같음
* 중요한 이유
  * 각각의 표본은 모집단의 특성을 나타내기에 부족하지만, 표본 평균의 분포가 n이 커지면 모집단의 특성을 나타낼 수 있게 됨
  * 즉, 통계량인 표본평균을 통해서 모집단의 모수인 모평균과 모표준편차를 추정할 수 있는 확률적 근거를 제서해주는 것임
  * 많은 통계적 방법이 정규성 가정에 의존하기 때문


<br>

# 💡 데이터 전처리
### ⬛ Unbalanced Data
불균형 데이터란 데이터 내 **각각의 클래스들이 차지하는 데이터의 비율이 균일하지 않고 한쪽으로 치우친 데이터**를 말한다. 일반적으로 불균형 데이터를 통해 모델을 학습할 시에는 다음과 같은 문제점을 가진다.  <br>
* 적은 수의 이상치에 편향된 분류 경계선이 학습됨에 따라 예측 단계에서의 오분류율이 높음
* 높은 정확도에도 이상 클래스에 대해서는 잘 분류하지 못해 모델 성능에 대한 왜곡을 불러일으킴

범주형/연속형 불균형 데이터 중 불균형 데이터에 대한 해결법은 보통 범주형 데이터에 대한 것들이 많다. (현실의 데이터들은 대체로 연속형 데이터가 많음에도 불구) <br>
불균형 연속형 데이터의 경우, 기존 불균형 범주형 데이터와 다른 특징을 보인다.
* 클래스 경계가 존재하지 않음 : resampling, reweighting 방법을 적용하기 어려움
* 타겟값끼리 연속성 및 유사성: 주변값의 분포에 따라 다른 수준의 불균형을 겪음
  * 이웃한 범위 내의 데이터가 많고 적을수록 불균형의 정도가 다름
* 특정 대상값에 대한 데이터가 아예 없을 수 있음
  * 주변 데이터를 통해 interpolation 또는 extrapolation 가능

그렇기 때문에 인접 데이터간 유사성과 커널 함수를 활용하여 불균형 문제를 해소하고자 하는 방법들이 많이 발생한다. (분포를 기반으로 소수 클래스에 해당하는 데이터를 증강시킴)<br>
* 대표 방법론
  * Label Distribution Smoothing(LDS): 레이블 공간 관점
  * Feature Distribution Smoothing(FDS): 특징 공간 관점
  * SMOGN(Synthetic Minority Over-Sampling Technique for Regression with Gaussian Noise)
    * Gaussian Noise의 기본 원리에 따라 기존 데이터를 훼손시키지 않으면서, 과대 추정될 수 있는 범위의 데이터는 축소시키고, 과소 추정될 수 있는 소수 데이터 범위의 데이터는 증폭시켜주는 오버샘플링 기법
    * python 라이브러리 형태 : smoter함수의 advanced mode를 사용하여 샘플링 해줄 것인지에 대한 세부인자 값들을 수동으로 설정할 수 있음
    * 이때 세부인자 설정 과정에서, 오버샘플링 데이터의 왜도 값을 비교하며 낮은 왜도 값을 가질 때의 세부인자를 선정할 수 있음

<br>

# 💡 모델링 개념
### ⬛ 머신러닝 종류
1. **지도학습(supervised learning)**
  * 데이터에 대한 정답(Y)을 주고 학습시키는 방법
  * 분류, 회귀
2. **비지도학습(unsupervised learning)**
  * 데이터에 대한 정답(Y)을 주지 않고 학습시키는 방법
  * 클러스터링, 오토인코더
3. **강화학습(reinforcement learning)**
  * 에이전트(Agent)가 주어진 환경(State)에 대해 어떤 행동(Action)을 취하고, 이로부터 보상(Reward)을 얻으면서 학습을 진행하는 방식
  * 에이전트가 보상을 최대화하는 방식으로 학습이 진행

### ⬛ Holdout method
데이터셋을 train, test, eval set으로 분할하여 사용하는 모델 선택 방법이다. <br>
train set으로 모델을 훈련하고, eval set은 모델 선택에 사용하여, test set으로 모델 훈련 뒤 성능 평가에 사용된다.

### ⬛ 1D-CNN, 2D-CNN, 3D-CNN 차이점
CNN 모델은 1D, 2D, 3D로 나뉘는데, 일반적인 CNN은 보통 이미지 분류에 사용되는 2D를 통칭한다. <br>
여기서 D는 차원을 뜻하는 dimensional의 약자로, **인풋 데이터 형태에 따라 1D, 2D, 3D 형태의 CNN 모델이 사용**된다.
* 즉, **입력 데이터의 차원**에 따라 Conv1D, Conv2D, Conv3D를 사용함
* **합성곱을 진행할 입력 데이터의 차원**을 의미 → 합성곱 진행 방향을 고려해야함
* 1D, 2D, 3D 기준 : 합성곱이 진행되는 방향 + 합성곱의 결과로 나오는 출력값 
  * Conv1D : 합성곱 진행 방향이 한 방향(가로)
    * 합성곱을 진행할 입력 데이터의 차원은 1
    * sequence 모델과 자연어 처리(NLP)에서 주로 사용
    * NLP : 각 단어 벡터의 차원 전체에 대해 필터를 적용시키기 위함
  * Conv2D : 합성곱 진행 방향이 두 방향(가로, 세로)
    * ex. `tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3))`
    * 차원이 (150, 150, 3)인 image에 대해 두 방향으로만 합성곱을 진행하겠다는 뜻
    * 150x150 이미지에 3채널이므로, 150x150 matrix에 대해 합성곱을 총 3번(R, G, B) 진행
    * 즉, 합성곱을 진행할 입력 데이터의 차원은 2이다.
    * 컴퓨터 비전(Computer Vision, CV)에서 주로 사용
  * Conv3D : 합성곱 진행 방향이 세 방향(가로, 세로, 높이)
    * 의료(CT 영상) 분야와 비디오 프로세싱에서 주로 사용

### ⬛ kernel, filter 차이점
필터는 여러개의 kernel로 구성되어 있으며, 개별 kernel은 필터내에서 서로 다른 값을 가질 수 있다. kernel의 개수가 바로 channel의 개수이다.
* kernel : sliding window하는 영역에서의 크기 (ex. 4x4)
* filter : 실제로 kernel이 weighted sum하는 영역의 크기 (ex. 4x4x3)
* D : kernel이 sliding하는 dimension 크기
* feature map : 입력 이미지와 필터 간의 convolution 연산의 출력

![image](https://github.com/ysjang0926/ysjang0926.github.io/assets/54492747/92a5e715-8e55-40ad-a261-2d3974f41f31)

#### Kernel size
Convolution Filter를 Kernel로도 지칭
* kernel size(크기)라고 하면 면적(가로x세로)을 의미하여 가로와 세로는 서로 다를 수 있지만 보통 일치시킴
* kernel 크기가 크면 클수록 입력 feature map(또는 원본 이미지)에서 더 큰(또는 더 많은) feature 정보를 가져올 수 있음
* 큰 사이즈의 kernel로 convolution 연산을 할 경우 훨씬 더 많은 연산량과 파라미터가 필요함


### ⬛ 모델 최적화
* Hyperparameter Tuning
  * Hyperparameter : 모델을 생성할 때, 설정할 수 있는 모델 변수
  * Grid Search : 가장 좋은 성능을 보이는 파라미터들의 조합을 선택하는 방법
* K-Fold Cross Validation
  * 과적합을 막기 위한 모델 선택 방법
  * 전체 데이터셋을 k개로 나누고 한뭉치씩 돌아가며 테스트셋으로 지정하여 모델을 훈련시키는 방식
  * 데이터를 k개의 서로 다른 부분으로 나누고, 각각의 부분을 교차 검증(cross-validation)에 사용
  * 모든 데이터를 활용하여 k개의 학습과 검증 과정을 거치며 신뢰도 높은 모델을 만들어냄
  * k-fold ensemble : k개의 예측을 평균하여 최종 결과를 도출함

### ⬛ Regularization
과적합을 막기 위해 특정 가중치가 너무 커지지 않도록 제한하는 방식으로 모델 복잡도를 줄이며, L1 규제와 L2 규제가 있다. <br>
L1에 비해 L2는 이상치나 노이즈가 있는 데이터에 대한 학습에 좋으며, 특히 선형 모델의 일반화에 좋다.
1. L1 규제 : cost function 식에서 가중치의 절대값을 더해줌
2. L2 규제 : 가중치의 제곱값을 이용

### ⬛ Activation function
딥러닝 네트워크에서는 노드에 들어오는 값들에 대해 곧바로 다음 레이어로 전달하지 않고 주로 비선형 함수를 통과시킨 후 전달한다. <br>
이때의 함수가 activation function이며, 노드로 들어오는 신호에 대해 신호를 전달할만큼 의미가 있는지 없는지(가중치가 큰지 안큰지) 판단해주는 함수이다. <br>
대표적으로 시그모이드 함수, ReLU 함수, tanh 함수 등이 있다.
* ReLU 함수 : 음수는 0으로 통과를 시키고 양수는 그대로 통과

### ⬛ Pooling
pooling 목적은 **특징을 강화**하는데 있다. 사용법은 위의 convolution layer랑 비슷하지만, **값들 중 특정값만 유지하고 나머지 값은 버린다**고 생각하면 된다. <br>
Pooling layer의 종류에는 max pooling, average pooling, overlapping Pooling이 있다. 
* max polling
  * 계산양이 감소하기 때문에 연산부하가 줄어듦
  * size를 줄이는 것이기 때문에 필연적으로 오차가 발생하므로 오버피팅을 약간 줄여줌
  * back propagation 시 복원이 힘들어서 너무 많이 넣으면 안됨

### Dropout
신경망 모델을 만들 때 생기는 문제점 중에 대표적인 것이 과적합(Overfitting)이다. <br>
보통 과적합 문제는 **정규화(Regularization)** 방법으로 많이 해결하고 정규화 방법 중 대표적인 것인 Dropout이다. <br>
→ 학습 데이터에 과정합되는 상황을 방지하기 위해 **학습 시 특정 확률로 노드들의 값을 0으로 보게 된다**. [주의] 이러한 과정은 학습할 때만 적용되고 예측 혹은 테스트할 때는 적용되지 않아야 한다. 

### ⬛ Class Imbalance
대부분의 이미지는 negative sample(background) 픽셀수가 많고, positive sample(object)이 적기 때문에 불균형이 존재한다.

### ⬛ 성능평가지표
모델의 성능을 평가할 수 있는 지표로, 정확도/정밀도/재현율/F-score 등이 있다.
1. Accuracy
  * True를 True라고, False를 False라고 옳게 예측한 비율
  * 모든 데이터 중에서 모든 'True'들의 비율
2. Precision
  * 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율
3. Recall
  * 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율
4. F1-score
  * precision과 recall의 조화평균
  * label의 수가 불균형적일 때, 모델의 성능을 비교적 정확히 평가할 수 있음

### ⬛ AI모델 정확도 vs. 속도 vs. 비용
AI 모델의 정확도를 높이다 보면, 연산량이 늘어나 속도가 느려지고 비용도 증가하여 사용자에게 안좋은 경험을 제공할 위험이 크다. <br>
그렇기 때문에 속도/비용도 함께 고려한 효율적인 모델을 만드는 것이 중요하다.


### ⬛ 자동 재학습 파이프라인
모델의 성능 저하는 학습 데이터와 실제 추론 데이터의 분포 차이에 의해 발생한다.(Data Drift) <br>
그렇기 때문에, 모델 학습과 평가 시에 사용한 데이터 세트과 최신 데이터의 분포가 상이하다면 정확한 추론 결과를 얻을 수 없다. <br>
실제 추론 환경에서 성능 저하 현상을 방지하기 위해서는 최신 데이터 학습이 필요하다. 이를 대응하기 위해 데이터 수집, 정제, 학습, 배포 과정을 자동을 실행하는 파이프라인이 중요하다.
* 모델별 특성에 따라 주기와 데이터 학습 window 기준, 학습 데이터 정제 방법이 달라 별도의 학습 파이프라인 동작 필요
* 모델 배포 이전에 평가 과정을 파이프라인 내에서 수행하여, 자동으로 최신 데이터를 반영하면서 선능 유지하는 것이 가능하도록 해야함

<br>

# 💡 모델 설명
### ⬛ XGBoost 알고리즘
gradient boosting 알고리즘을 기반으로 한 ML 모형이다.
* gradient tree bosting : 간단하지만 정확도가 낮은 모델의 결과값을 결합하여 빠른 속도로 높은 정확도에 도달하는 알고리즘

병렬 처리를 통해 학습 속도가 빠르고 과적합에 대한 규제(regularization) 기능을 갖음과 동시에 ML 모형 중에서는 비교적 높은 예측 성능을 발휘한다.
* 데이터 분포가 고르지 않은 데이터 양이 적은 부분에 대한 과적합을 방지
* 빠른 학습을 통해 최신 데이터 학습 주기가 유연하게 변동 가능
* CatBoost, LightGBM 대비 복합적인 조건에서 발생하는 분석 조건에서의 예측력이 상대적으로 높은 정확도를 나타냄

주로 Y값에 영향을 미치는 변수의 설명력과 플랫폼 초기 구축 용이성을 위해 최적의 모형으로 사용되기도 한다.


### ⬛ LSTM + Autoencoder (LSTM-AE)
AutoEncoder는 원본 데이터를 특징백터(feature)로 압축하고 의미 있는 표현으로 인코딩한 다음 복원(reconstruction)시켜 복원된 데이터가 원본 데이터와 최대한 유사하도록 만든 신경망이다. 그렇기 때문에 labeled 데이터 없이 학습이 가능한 unsupervised 방법이다. <br>
이때 **AutoEncoder에 LSTM 구조를 추가하여 sequence 또는 time-series 데이터를 Self-Supervised 방법으로 학습**하는 것을 LSTM AutoEncoder라고 부른다. <br>
→ 인코더와 디코더에 LSTM 신경망을 적용한 Autoencoder
* Encoder : sequence 데이터를 압축하는 LSTM 모듈
  * sequence 데이터는 차례대로 LSTM 모듈의 input으로 사용되어 feature 벡터로 변환됨
  * feature 벡터는 sequence 데이터를 압축한 형태로 이미지의 모습과 이미지의 이동방향 등의 정보가 포함되어 있음
* Reconstruction Decoder : Encoder에서 생성된 feature 벡터를 이용하여 input sequence 데이터를 복원하는 LSTM 모듈
  * input sequence의 반대 방향으로 진행
* Prediction Decoder : Encoder에서 생성된 feature 벡터를 이용하여 input sequence 이후 나올 미래의 이미지 sequence를 생성하는 LSTM 모듈
![image](https://github.com/ysjang0926/ysjang0926.github.io/assets/54492747/e9fef3a3-b8d4-40c7-857e-a892180c9683)



Autoencoder는 일반적인 용도인 차원축소(dimension reduction) 뿐만 아니라 이상 탐지(anomaly detection) 분야에도 사용된다.
* 만약 정상 데이터로만 Autoencoder를 학습하면 원본 데이터와 복원 데이터의 차이인 복원손실 또는 복원오차는 매우 작을 것임
* 하지만 학습된 모델에 학습에 사용한 데이터와 상이한 특성을 갖는 데이터가 입력된다면 이 데이터는 학습되지 않았기 때문에 복원된 데이터는 원본 데이터와 많은 차이가 발생할 것
  * LSTM-AE의 성능은 원본 입력 시퀀스와 복원된 시퀀스간의 차이로 평가됨
* 복원오차가 정해진 임계치(threshold)를 넘게 되면 이상으로 판정


LSTM AutoEncoder는 reconstruction task와 prediction task를 함께 학습함으로써 각각의 task만을 학습할 경우 발생하는 단점을 극복할 수 있다. 두가지 task를 함께 학습함으로써 모델이 모든 정보를 저장하지 않고 중요정보(이미지 모습, 이동방향 등)를 feature에 저장하도록 유도할 수 있다. 또한 Sequence 데이터의 모든 시점 정보를 활용하여 학습함으로써 모델이 쉽게 학습할 수 있도록 돕는 역할을 한다.
* reconstruction task만을 수행하여 모델을 학습할 경우
  * 모델은 input의 사소한 정보까지 보존하여 Feature 벡터를 생성함
  * 즉 사소한 정보가 저장될 수 없게 Feature 벡터의 크기를 작게 설정하지 않으면 과적합(overfitting)이 발생하는 단점이 존재함
* prediction task만을 수행하여 모델을 학습할 경우
  * 모델은 input의 최근 sequence 정보만을 이용하여 학습함
  * 일반적으로 prediction에 필요한 정보는 예측하기 전 시점에 가까울수록 상관관계가 높기 때문임
  * 따라서 과거 시점의 정보를 활용하지 못하는 단점이 존재함



### ⬛ DNN
다층 퍼셉트론(Multi-Layer Perceptron)으로, 인공 신경망의 한 종류이다.
* 신경망 모형 중 정형데이터 처리에 적합
* 사람의 신경망과 비슷한 형태로 입력층, 은닉층, 출력층으로 구성됨
  * input layer : X값 / hidden layer : X값 데이터 특징 추출 / output layer : Y값
  * ex) 1개의 input layer & 각 5개, 5개 뉴런으로 이루어진 2개의 hidden layer & 1개의 output layer
* 각 층의 노드들은 가중치와 활성화 함수를 통해 입력 신호를 변환하고 처리함
* 파라미터
  * 파라미터 선택 방법 : 각 파라미터는 MAE 값이 가장 낮은 지점의 형태 선택
  * ex) activation function : relu, linear / Loss function : MAE / Optimizer : Adam / Learning Rate : 0.001 / Epoch : 500 / Batch size : 6

### ⬛ CNN
CNN은 2가지 특징이 있다. <br>
→ 특징을 추출하는 feature extraction & feature extraction를 통과한 이후에 결과값을 도출해 주는 Classification
* Feature extraction : Convolution layer와 Pooling layer가 섞여 있는 것
* Classification : fully-connected layer로 이루어진 것

### ⬛ 1D-CNN (1 Dimensional Convolution Neural Network)
* 시간에 따라 데이터가 구성되는 시계열 데이터에 적합함
* 1차원 Convolutional Neural Network으로, 인공 신경망의 한 종류
* 주로 시계열 데이터나 순차적인 데이터에서 패턴을 감지하고 학습하는데 좋은 성능을 보임 (변수 간의 지엽적인 특징을 추출)
* 각각의 층에서 입력 데이터의 부분적인 패턴을 파악하는 일종의 filter 작용을 거침
* Layer
  * Convolution layer(Conv1D) : 필터를 통해 이미지 데이터 특성을 추출하고 패턴을 파악 (필터 : 이미지에서 작은 영역을 선택하여 해당 영역의 정보 추출)
  * Pooling layer : 특징 강화 + 이미지 크기 줄임
  * Fully Connected layer(FCN) : 추출된 특징을 이용하여 이미지를 분류
  * dropcout layer : 과적합 방지
  * ex) conv1d : filters=32, kernel_size=100 / Maxpooling1D : pool_size : 2 / Dense : units : 10 / Dropout : 0.1

### ⬛ EfficientNet
모델 성능을 극대화하기 위하여, network의 depth(깊이), width(필터 수), 이미지 resolution 크기를 최적으로 조합한다. <br>
이때 EfficientNet은 Compound Scaling을 활용하여 타 모델(vgg16, resnet50)보다 더 적은 연산량으로 좋은 성능을 낼 수 있다. 
* Model Scaling 방법
  * Width Scaling : layer 너비 넓히기 → channel 증가
  * Depth Scaling : layer수 늘리기
  * Resolution Scaling : input의 해상도를 높이기
  * Compound Scaling : width, depth, resolution을 동시에 고려
EfficientNet B0~B7가 존재하며, B1~B7은 B0에서 depth, width를 증가시켜서 모델을 생성한다.

### ⬛ SSD(Single Shot Multibox Detector)
장점과 단점은 다음과 같다.
* 장점1) 실행 속도 fast : 단일 신경망을 사용하여, 한번의 순전파로 객체를 감지하므로 빠른 실행속도 가능
* 장점2) 다양한 크기의 박스 생성 : 다양한 크기의 박스를 동시에 예측하여 다양한 크기의 객체 처리 가능
* 단점1) imbalanced 낮은 예측 : 작은 물체에 대한 정확도가 떨어지며, class imbalance 취약

### ⬛ Faster R-CNN
장점과 단점은 다음과 같다.
* 장점1) 정확한 객체 위치 예측 : RPN(Region Proposal Network)을 통해 정확한 객체 위치를 예측
* 장점2) 높은 정확도 : 정확한 객체 감지에 적합
* 단점1) 비교적 느린 실행 속도 : 여러 단계의 network를 거쳐서 객체를 감지하기 때문에, 실행 속도가 상대적으로 느림
* 단점2) 높은 계산 비용 : 실시간 응용에 적합하지 않음

### ⬛ RetinaNet
One Stage Detector의 빠른 detection 시간의 장점을 가지면서 One Stage Detector의 detection 성능 저하 문제를 개선한 모델이다. <br>
* 수행시간은 YOLO나 SSD보다 느리지만, Faster R-CNN보다 빠름
* 수행성능은 타 dectection 모델보다 뛰어나며, 특히 one stage detector보다 작은 object에 대한 detection 능력이 뛰어남
Focal Loss와 Feature Pyramid Network 특징이 있다.
* Focal Loss : Cross Entropy를 대체한 Loss Function
  * 어려운 샘플 or 오분류하는 픽셀값에 가중치를 주어, 모델이 객체에 더 집중하도록 하여 class imbalance 해결
* FPN(Feature Pyramid Network) : backbone에 적용
  * 다양한 크기 및 해상도를 가진 물체에 강건한 특징 추출

<br>

# 💡 분석 케이스 접근
### ⬛ 분석 정의
* 분석하고자 하는 Y에 대한 정확한 정의 필요 + Y에 영향을 주는 X 인자 또한 필요
  * 이에 따른 데이터 수집 주기와 측정 방식에 대한 설명 요청
* 분석 시나리오 기반, 현장에서 생각하는 가설을 데이터로 하나씩 검증하는 방식

### ⬛ 예측 모델링
* 딥러닝 모델을 활용한 제품 물성 예측
  * 목표 KPI 잡기 (ex. 제품 스펙 범위의 20%)
  * For 신뢰성 높은 모델 확보, 다양한 지표를 사용하여 모델 평가 필요

### ⬛ 이미지 분류
* 프로세스 : 결점 발생 이미지 라벨링 → 이미지 전처리 및 분할 → Train Set 이미지로 모델 학습 → Valid Set 이미지로 모델 최종 성능 평가
* 3가지 방식 존재
  1. Image Classification : 이미지에 있는 물체가 어떤 class(category)인지 class를 분류
    * 주로 CNN 기반의 모델을 사용
  2. Object Detection : 이미지에 있는 모든 물체의 Bounding Box로 탐지 후, 분류
    * Localization : 이미지 내 object에 bounding box를 생성한 후, 위치 정보 출력 및 분류
    * Detection : 이미지 내 multi-object의 위치 정보 출력 및 분류
  3. Image Segmentation : 이미지에 있는 모든 물체의 edge를 찾은 후, 분류
* 결점 이미지 특징 : gray-scale 이미지

### ⬛ 최적 조건 도출
* 목표 : 수명에 영향을 주는 요인을 찾고, 수명을 예측하여 생산 수율 향상을 하고자 함

### ⬛ 프로덕트 분석
데이터를 뭉뚱그려서 봤을 때와 쪼개서 봤을 때의 결과가 다를 수 있고, 기간을 어떻게 보느냐에 따라 해석이 달라지기도 해서 항상 여러 방면에서 종합적으로 분석해서 결론을 내야한다. <br>
그렇기 때문에 분석 결과에 대한 코멘트를 남기면서, 최대한 편향이 없는 결과가 공유될 수 있도록 노력해야한다.
1. 서비스 분석 관점
  * 서비스단에서 발생하는 문제를 정의하고 가설을 세운 후, 그 가설을 서비스 데이터로 검증
  * 그 결과를 토대로 서비스가 개선되는 방향으로 의사결정을 할 수 있도록 도움
2. 모델링 관점
  * 모델이 시장의 수요 공급에 따라 탄력적으로 움직이고 있는지 트래킹 하기 위한 지표 개발
  * 모델의 pain-point를 찾아 개선안을 제시하여 모델을 고도화
3. 데이터 관리 운영 업무
   * 기획이나 사업 부서에서 서비스 데이터를 편하게 볼 수 있도록 대시보드 생성
   * 그에 필요한 마트 테이블도 만들어 제공
   * 원천 데이터에 변화가 있을 때 마트테이블에 이를 반영하거나 수정하는 운영성 업무 진행

### ⬛ 고객 세그멘테이션 방법
![image](https://github.com/ysjang0926/ysjang0926.github.io/assets/54492747/dc7d386b-3a25-424e-9ff9-2b8f75f99b32)

#### 1단계. 특성기반 고객 세그먼테이션
빠르게 고객 세그먼테이션을 활용할 수 있는 방법이며, 고객 세그먼테이션을 할 때 기준이 필요함
1. 인구통계학적 세그먼테이션 : 사용 기기(e.g. 안드로이드, IOS, 태블릿 등), 나이, 성별과 같은 공통된 간단한 특성을 바탕으로 고객을 여러 그룹으로 나눔
  * 빠르게 세그먼테이션을 진행할 수 있는 가장 원초적이고 널리 알려진 간단한 방법
  * 고객 데이터의 개인정보가 불충분하거나 법적인 사용 제한 또는 고객사 데이터인 경우 용이
  * 데이터가 마스킹 되어있을 경우, 제한적인 데이터를 활용해야할 상황에 심플하게 활용할 수 있음
2. 위치적 세그먼테이션 : 고객을 국가, 지역, 도시와 같은 위치 및 장소에 따라 여러 그룹으로 나눔
  * 글로벌 지역 단위일 경우, 지역 및 도시별로 고객의 특성이 다르기 때문에 활용할 수 있음
  * 도메인 : 모빌리티, 배달 산업과 같은 GIS기반 위치가 중요한 외부 요인인 O2O 산업에서 활용 가능
  * 활용 방법: 서울 특별시 기준으로 Q-GIS 또는 Uber H3 활용해서 나누어볼 수 있음
3. 심리적 세그먼테이션 : 고객을 라이프스타일, 관심사, 가치 및 태도에 따라 여러 그룹으로 나누며, 이커머스 산업에서 많이 활용될 수 있음
4. 고객 행동 세그먼테이션 : 프로덕트 관점에서 고객의 웹/앱 로그, 사용 패턴, 브랜드 로열티 및 충성도, 마케팅 채널에 대한 반응률을 기준으로 고객을 여러 그룹으로 나눔
  * 고객이 주로 사용하는 마케팅 채널별(이메일, 앱푸시, 문자 등) 이용에 따라 그룹을 나눌 수도 있음

#### 2단계. 마케팅 기법 고객 세그먼테이션
* RFM 고객 세그먼테이션
  * 고객별로 얼마나 최근에, 얼마나 자주, 얼마나 많은 금액을 지출했는지에 따라 고객을 그룹으로 나눌 수 있는 마케팅 RFM(Recency, Frequency, Monetary)기법
    * R(Recency) 구매의 최근성: 고객이 얼마나 최근에 상품을 구입했는가?
    * F(Frequency) 구매 빈도: 고객이 얼마나 자주 상품을 구입했나?
    * M(Monetary) 구매 규모: 고객이 구입했던 총 상품 금액은 얼마인가?
  * 장기적으로 고객을 고정해서 활용해야할 경우 용이함

#### 3단계. 예측 고객 세그먼테이션 : ML 모델 활용
![image](https://github.com/ysjang0926/ysjang0926.github.io/assets/54492747/b73ae49c-3f93-4b89-bf2a-8544539f3be4)

1. 비지도 학습
  * 클러스터링 기법(k-means clustering, k-medoids clustering 등) : 도메인에 맞게 군집 알고리즘을 활용
2. 지도 학습
  * 고객 행동 예측 labeling하거나 고객 행동 데이터 예측을 바탕으로 고객이 어떤 행동을 할지 예측을 하여 분류

#### 4단계. 개인화 추천 고객 세그먼테이션 : 추천시스템 활용
데이터가 충분히 쌓였고 프로덕트의 유저가 많을 단계에 고객을 세분화하고 개인화 추천 타깃팅이 용이하다. 데이터를 수집할 수 있는 여건에 따라 컨텐츠 기반 필터링, 협업 필터링 등 추천시스템을 활용해서 만들 수 있습니다.
* 도메인 : 컨텐츠 산업(영화, 음악, 웹툰) 또는 이커머스 산업에서 컨텐츠, 이미지를 활용하여 나누어볼 수 있음
* 참고 예시 자료
  * [픽코마 웹툰 개인화 홈추천에서 클러스터링 타깃팅으로 MAB를 통해 웹툰 후보작품을 랭킹화한 내용](https://www.youtube.com/watch?v=RK3-aNWveMs)

#### 5단계. 혼합 고객 세그먼테이션
로직 트리처럼 고객 세그먼테이션 분석 목적에 맞게 위 4단계 방법 중 다음과 같이 몇가지 방법들을 혼합해서 활용할 수 있다. 
* 위치적 세그먼테이션(지역) + 추천시스템
* RFM + 비지도 학습
고객 세그먼트 프로젝트를 마치면 고객을 기준에 맞게 그룹별로 잘 나누었는지를 분석하는 단계도 중요하다. 예상치 못한 아웃라이어가 있을 수도 있고, 비중이 적지만 리텐션 또는 매출이 큰 그룹이 존재할 수도 있다. <br>
세그먼트별로 나눈 결과를 바탕으로 정성적인 부분 또는 도메인을 고려하여 적용해볼 수 있으며, A/B 테스트를 통해 사후 효과를 확인해볼 수 있다.


### ⬛ A/B Test
A/B Test는 **웹 사이트 방문자를 임의로 두 집단으로 나누고, 한 집단에게는 기존 사이트를 보여주고 다른 집단에게는 새로운 사이트를 보여준 다음, 두 집단 중 어떤 집단이 더 높은 성과를 보이는지 측정하여, 새 사이트가 기존 사이트에 비해 좋은지를 정량적으로 평가하는 방식**을 말한다. <br>
여기에서 성과란 새 사이트가 목표로 했던 바에 따라 다른데, 보통은 **회원 가입율, 재방문율, 구매전환율 등의 지표**를 본다. <br>
과학 혹은 의학에서 무작위비교연구(RCT; Randomized-controlled trial)라 불리는 방법을 인터넷 마케팅에 적용한 것이라고 생각하면 된다. 주로 웹사이트의 마케팅과 관련하여 많이 쓰이지만 디자인, 인터페이스, 상품 배치 등을 개선하기 위해서 사용하기도 하며, 웹사이트가 아닌 모바일 앱, 게임 등의 분야에서도 활용된다.

#### RCT vs. A/B Test 차이점
* **RCT**
  * 오프라인 환경
  * 표본 확보가 어렵고 비용이 큼
  * 반복 실험이 어려움
  * 실험 설계가 복잡
  * 반복측정 분산분석, 요인 설계, 분할구 설계
* **A/B Test**
  * **온라인 환경**
  * 표본 확보가 쉽고 비용이 작음
  * **반복 실험이 용이**
  * 실험 설계가 매우 간단

#### A/B Test 설계 과정
A/B test에서는 A와 B 디자인에 노출되는 사람들을 무작위로 나누어 최대한 실험 환경과 비슷하도록 **유사 실험 환경을 구축**한다.
1. 사용자 및 지표 선정
2. 실제 환경 선정
3. 사용자 무작위 선정
4. 사용자에게 무작위로 A와 B 노출
5. 결과 분석 및 검증

#### A/B 테스트의 결과를 잘 분석하기 위해서 고려해야할 점
샘플 사이즈와 검정력(Power), 효과 크기(Effect size)를 이해하는 것이 매우 중요하다. 이를 제대로 알아야 맹목적으로 p-value에만 의지하는 결과 해석을 피할 수 있다. <br>
가설 검정과 더불어서 A/B 테스트는 변하는 세상에 대응하기 위한 노력이 수반되어야 한다. 기존의 실험 환경과는 다르게 시시각각 변하는 환경에서 실험을 수행하기 때문에 이에 대비한 장치가 필요하다. **MAB**나 **Bayesian A/B test**가 좋은 대안이다.

#### 신뢰구간을 이용한 A/B 테스트
만약 다음과 같은 Test를 1주일 간 진행하였다고 가정한다.
> 방문객은 무작위로 Variation A 또는 Variation B에 할당됨
> Variation A에 할당된 방문객은 특정 화면에 노출되고, Variation B에 할당된 방문객은 또 다른 특정 화면에 노출됨
> 실험의 목표 : 과연 어떤 화면에 노출되었을 때 방문객이 더 많은 구매를 하는지 알아보는 것
> 비교 : 방문객 당 평균 구매액(Average Purchase Per Visitor)을 계산

이때 조심해야할 점은 평균 구매액의 점 추정(point estimation)만으로 실험 결과를 비교하는 것이다.
* 추정 : 불확실한 무언가를 알기 위한 과정 → 여기서는 모집단의 평균 구매액에 해당
* 알고 싶은 것 : 앞으로 방문할 모든 방문객의 평균 구매액
* 표본 집단 : 1주일 동안 실험에 참여한 방문객들
* 표본 평균 : 방문한 방문객들의 평균 구매액
* 점추정 : 모집단의 평균(모평균) 값을 알기 위해서 표본 평균을 활용하는데, 단 하나의 값으로 이를 추정하는 과정 → 평균 표본 = 모평균의 점추정값

실험에서 얻은 데이터를 최대한 활용해서 현재 선택할 수 있는 가장 좋은 의사 결정을 내리는 것이 중요하다. 실험 데이터의 분산 값을 활용하여 평균 구매액의 점 추정 값이 아닌 구간 추정 값을 계산한다.
* 문제 해결 핵심 : 매번 움직이는 표본 평균을 탓할 것이 아닌, 얼마나 많이 움직이는지를 평가하는 것
* 표준 오차(standard error) : 표본 평균의 변동성 → 표본 분산과 표본 집단의 크기로 알 수 있음 
![image](https://github.com/ysjang0926/ysjang0926.github.io/assets/54492747/a31e966a-8b6d-42ba-bf4e-37b43d6d3457)

표본 평균의 확률 분포는 표본 크기가 충분히 큰 경우에 정규 분포를 따르게 되는데, 이는 중심 극한 정리(central limit theorem)에 의해 증명된다. 이때 **표본 평균의 값과 표준 오차를 알면 정규 분포를 이용하여 표본 평균의 범위를 구할 수 있다**.
* ex. Variation A의 평균 구매액에 대한 95% 신뢰 구간이 (121.75,160.17)
* 95% 신뢰구간 = 여러 개의 다른 표본에서 신뢰 구간을 같은 방법으로 구할 경우 95%의 구간들이 모평균을 포함한다는 의미
* 해당 범위를 통하여 표본 평균의 범위가 대략 어느 정도인지 알 수 있음 → 표본 평균의 변동성을 알 수 있기 때문에 다른 표본과의 비교가 용이

두 개의 표본 평균의 신뢰구간을 비교하면 아래와 같으며, 각 Variation의 신뢰 구간이 겹치지 않는 것을 알 수 있다.
* 각 신뢰 구간이 겹치지 않는 경우, 두 표본 평균은 유의하게 다름(significantly different)
* A/B 테스트 결과로 이야기하면, B의 평균 구매액이 A의 평균 구매액보다 큼

이때 주의할 것은 그 역은 성립하지 않는다는 점이다. 신뢰 구간이 겹치지 않는다면 두 표본 평균은 유의하게 다르다고 할 수 있지만, 신뢰 구간이 겹치는 경우에는 그 정도에 따라서 표본 평균이 유의하게 다르거나 다르지 않을 수 있다. <br>
이럴 때는 가설 검정을 제대로 할 필요가 있으며, 신뢰 구간을 이용한 통계적 유의성 검토는 빠르지만 정확하지 않은 방법임을 명심해야 한다.
* A/B 테스트 결과를 빠르게 비교할 수 있는 방법으로 신뢰 구간을 활용할 수 있음
* 특히, 표본 평균의 신뢰 구간이 겹치지 않는다면 통계적으로 유의한 차이가 있다고 봄
* 그러나 이는 빠르게 결과를 확인하는 용도이지 결코 가설 검정을 대체할 수는 없기 때문에, 실무에서 유용하므로 잘 이해하고 쓰는 것이 중요함

#### Bayesian A/B test
1. 전환율 검정 (Conversion Testing)
  * 관측 데이터 : 각 방문자마다 A나 B를 통해 구매했는지 여부
  * 가설 : A보다 B가 구매로의 전환율이 높을 것이다
  * 가능도(Likelihood) = 우리가 관측하는 데이터의 함수 : 이항 분포(Binomial Distribution)
    * 이항분포 확률변수 : n명의 사람 중 어떤 event를 성공한 사람수
  * 사전 분포(Prior) : 베타 분포(Beta Distribution)
    * 베이지안은 모수에 사전 분포를 가정해 모수의 실제값이 하나가 아니라, 사람들의 믿음(belief)에 따라 달라지는 분포 형태를 띠고 있다고 생각함
    * 여기서 관심 모수는 Pa, Pb인 전환율이기 때문에, 이에 맞는 사전 분포를 가정하는 것이 중요함
    * 베타 분포를 따르는 확률변수는 항상 (0,1) 사이의 값이기 때문에, 성공 확률의 분포의 자연스러운 가정이 됨
    * 특별한 사전 지식이 없을 경우 무정보 사전 분포(Non-informative prior)를 주는 것이 일반적임
    * 무정보 사전 분포는 말 그대로 정보가 없는 사전 분포로, 베타 분포의 경우 Beta(1,1)에 해당함
  * 사후 분포(Posterior) : 베타 분포
    * 사전 분포와 가능도를 각각 베타와 이항분포로 설정하는 것의 장점 : 켤레성(Conjugacy) 이용 → 사후 분포(Posterior Distribution)를 구했을 때, 그 형태가 사전 분포와 같은 분포일 때 켤레성을 띤다고 말함
    * 가능도가 이항 분포 Binomial(n,p)고 사전 분포가 Beta(α,β)일 경우 : 사후 분포는 Beta(α+x,β+n-x)를 따르게 됨
    * 사후 분포에서 난수를 생성한 후 평균을 구하면 각 전환율의 추정치를 구할 수 있음
  * 시뮬레이션 결과
    * `print((posterior_samples_A > posterior_samples_B).mean())` : posterior mean 결과값 = 0.31355
    * A 디자인의 전환율이 B 디자인의 전환율보다 높을 확률이 31.36% = B 디자인 전환율이 A의 전환율보다 높을 확률이 68.64%

2. 기대 수익 분석 (Expected Revenue Analysis)
  * 가정 : 어떤 웹사이트의 기대 수익 : E[R] = 79p1 + 49p2 + 25p3 + 0p4
  * 가정 :  p1, p2, p3, p4 = 각각 $79, $49, $25 가격 플랜을 선택할 확률 & 아무 플랜을 선택하지 않을 확률
  * 가능도 : 다항 분포((Multinomial Distribution) → 이항 분포 확장판
  * 사전 분포 : 디리클레 분포 (Dirichlet Distribution) → 베타 분포 확장판
  * 사후 분포 : 디리클레 분포
    * 가능도가 다항분포 (x1,x2,x3,x4)이고 사전 분포가 Dirichlet(1,1,1,1)일 경우 : Dirichlet(1+x1,1+x2,1+x3,1+x4)


#### MAB (Multi-Armed Bandits)
테스트에는 **탐색-활용 교환(Exploration-Exploitation tradeoff)**이 존재한다.
* 탐색(Exploration) : 가장 나은 대안을 찾기 위해 테스트하는 과정
* 활용(Exploitation) : 테스트를 중단하고 결정된 대안을 선택하는 것
* 테스트를 많이 하면 많이 하는대로 기회 비용 문제가 발생하고, 안하면 안하는대로 신뢰성의 문제가 발생

위의 문제를 체계화한 것이 **MAB(Multi Armed Bandit)** 이다. MAB가 똑똑한 이유는 **탐색과 활용을 최적화**하여, **수익률을 극대화**하기 때문**이다. <br>
MAB에서 자주 쓰이는 용어는 다음과 같다.
  * 행동(Action): MAB에서 선택된 대안 (ex. A/B 테스트에서 A안, B안)
  * 보상(Reward): 한 번의 행동에 따른 수치화된 결과 (ex. 클릭, 구매)
  * 가치(Value): 행동으로 인한 기대 보상
MAB에서는 모든 행동이 순서대로 발생한다고 가정한다. 그 순서에 따라 시점 t의 행동을 At라 하고, 행동에 따른 보상은 Rt로 표기한다. 또한, 행동 a의 가치는 q*(a), 시점 t에 추정된 가치는 Qt(a)라 한다.

MAB에서 봐야할 알고리즘은 총 4개가 있다.
1. 그리디(Greedy)
  * 현재 시점 t까지 기대보상 Qt(a)를 최대화하는 행동 At를 선택 → 위험성 ↑
  * 탐색(exloration)을 충분히 하지 않고, 활용(exploitation)을 너무 많이 하는 알고리즘
  * 그렇기 때문에 좋은 알고리즘은 아님

2. 입실론-그리디(Epsilon-Greedy)
  * 그리디 알고리즘에서 탐색을 촉진하기 위해 보완된 알고리즘
  * 1-ϵ의 확률로는 그리디 알고리즘 & ϵ의 확률로 랜덤하게 선택
  * ϵ의 확률만큼 항상 무작위 탐색(Exploration)을 해야 하기 때문에, 최적의 object를 찾았더라도 시간이 지나면 최적 값과 멀어지는 결과를 초래할 수도 있음
  * 단점 : 랜덤성으로 인해서 최적값과는 멀어지기도 하고 탐색을 과도하게 하는 경향 有

3. UCB(Upper Confidence Bound)
  * 이 알고리즘의 아이디어는 추정된 가치 Qt(a)에서 일종의 신뢰 구간을 구해서 그 구간의 위쪽 신뢰 구간의 행동을 선택하는 것임
  * 활용과 탐색을 적절히 고려해서 선택하는 알고리즘

4. 톰슨 샘플링(Thompson Sampling)
  * 베이지안 방법으로, 관심 있는 모수에 대해 사전 분포를 정의하고, 관측된 값으로부터 사후 분포를 이끌어냄
  * 모평균과 모표준편차가 하나의 분포(=사전 분포)를 따른다고 가정하고 이를 데이터(= 가능도)에 따라 모평균과 모표준편차에 대한 분포를 업데이트(=사후 분포)하는 방식

#### A/B Test를 신뢰하기 어려운 케이스
1. 이전 실험에 사용한 **실험 모집단을 재사용**하고 있음
  * **발생하는 문제 : 잔류 효과(Carryover effect)**, ex.실험 종료 후에도 실험 효과가 약 3주간 잔존
  * **해결방법 : 실험을 진행할 때마다 Randomization → 새로운 실험 할당 시 재할당(Re-randomization) 통한 새로운 실험 집단 구성**
2. **사용자 단위의 지표를 사용하고 있지 않음 (실험 단위, 분석 단위 같지 않음)**
  * 실험단위 = 사용자 / 분석단위 = 분석에 사용되는 지표의 단위
  * **발생하는 문제 : 지표 분산에 Bias가 생김** → 표준편차(standard deviation)과 표준오차(standard error)를 혼동하면 안됨
  * 실험단위와 분석단위가 다르면 i.i.d. 가정이 깨짐 (independent and identically distributed)
  * 관심 지표가 1,000개라 하고, A/A Test 기간 내 관측한 지표들의 p-value를 구했을 때 histogram은 균일 분포를 띄어야함 → A/A Test 성공
  * **해결방법 : Delta Method를 통한 지표의 분산 추정 or Bootstrap sampling** 
3. A/A Test를 수행하고 있지 않음

A/A Test는 우리가 모르고 지나칠 수 있는 함정을 발견할 수 있는 유일한 무기이다.
* 실험 플랫폼에 대한 신뢰를 구축하는 데 매우 유용
* 분포 불일치와 플랫폼 이상을 포함한 문제를 발견하기 위해 다른 실험과 병행해서 계속해서 A/A Test를 실행하는 것을 추천


<br>

# 💡 기술 질문
### ⬛ 매출이 감소했다고 하면 어떻게 접근하여 분석하겠는가?
매출의 증감을 표현할 수 있는 다양한 지표(Index)를 생성(기존+신규)하여, 사업부/상품/기간 단위로 매출 감소 영역을 Sensing하고 상세 분석을 통해 매출 감소 원인을 파악하겠습니다.

### ⬛ 배치 파이프라인의 Tool인 Airflow란?
Apache Airflow는 프로그래밍 방식으로 워크플로우를 작성, 예약 및 모니터링하는 오픈 소스 플랫폼입니다. 정확한 시간에, 정확한 방법으로, 정확한 순서대로 실행하게 해주는 오케스트레이터라고 볼 수 있습니다.

### ⬛ 코로나와 같은 특수한 상황에 대해 어떻게 예측할 것인가?
과거에 코로나라는 특수한 전염병에 사례와 비슷한(메르스, 사스) 전염병의 데이터를 수집한 데이터를 활용하여 다양한 파생변수를 생성하고, Feature로 사용할 것 입니다. 크롤링을 통해 검색어 증감을 분석하여 사전 Issue Alert 전달하고, 앞선 대응을 진행할 것 입니다.

### ⬛ RFM 방법론의 효과가 좋지 않은데 어떻게 사용할 것인가?
어느 영역(R/F/M) 가중치를 더 두느냐에 따라서 모델의 가치가 결정된다고 생각합니다. 분석 목적에 맞는 가중치 설정을 통해 최고의 효과를 달성하는 방안을 고안한다면 RFM도 성능을 높일 수 있을 것이라고 생각합니다.

### ⬛ 2~3천개의 Feature가 있는데 어떻게 접근하겠는가?
모범 답안 NA 비율 및 Zero 비율, Outlier 확인후 의미없는 변수를 1차 제거하겠습니다. Correlation 분석을 통해 상관관계가 높은 변수를 제거(Y의 영향을 가장 많이 미치는 변수를 생존시킴)하고 만약 컴퓨팅 파워가 가능하다면 그리고 Tree 계열의 알고리즘을 사용한다고 가정했을 때, 최대한 많은 변수를 넣어서 진행하여 성능을 높이는 방향성으로 분석을 진행하겠습니다.

### ⬛ 데이터 분석 프로세스에서 가장 중요하다고 생각되는 단계는?
현업에 Needs를 파악하여 과제를 구체화시키는 단계와 분석된 결과를 현업에 적용시키는 단계가 가장 중요하다고 생각합니다. 과제 구체화와 현업 적용 단계를 놓치면 제대로된 분석결과를 기대할 수 없기 때문입니다.

<br>

# 💡 CRM 캠페인 최적화를 위한 모델링
> 광고 효율화 : 최소한의 예산으로 우리 광고에 적합한 유저를 찾아 효과적인 광고를 만드는 것
> 이때, 광고 효율화를 위해서는 타겟, 예산, 노출 위치, 광고 소재 등 고려해야할 포인트가 너무 많음

### ⬛ [ifkakao 발표](https://brunch.co.kr/@lulina724/31)
* 컨텐츠에 따라 데이터 특성이 다르기에 특정 컨텐츠는 빠른 피드백을 수집하는 것 중요


### ⬛ 페이스북 사례
* 데이터 : 사용자드르이 행동 패턴, 유저가 반응한 광고 유형 등
* 특정 시간, 노출 위치, 다양한 디바이스에 광고를 뿌려 보고 가자 좋은 경우의 수를 찾고 광고 노출 반복됨
* **약 7일 동안 50개의 테스트**를 머신러닝 기간으로 봄
* 캠페인 목표
  * ex) 트래픽 → 트래픽 건수가 50건 이상 나왔을 때 / 구매 목표 → 구매 결제가 50건 도달했을 때
  * 7일 기준 : 광고 세트를 생성하고 나서부터 7일이며, 그전에 수정을 하면 수정한 시점부터 다시 7일 시작

#### 최적화
최적화는 **내가 원하는 행동을 할만한 유저에게 광고를 노출시켜서 확률을 높이는 것**으로 정의할 수 있다.
* 내가 원하는 행동 = 나의 KPI
* 최적화를 위해서는 내 KPI가 무엇인지 아는게 중요
* 확률을 높인다는 것 = 전환율을 높이는 것

캠페인 목표별 게재 최적화 기준에서 가장 많이 실수하는 부분은 **랜딩 페이지 조회**와 **링크 페이지 조회**를 구분하는 것이다.
* 내가 만들어 놓은 상세 페이지인 랜딩 페이지를 클릭하는 것이 링크를 클릭하고 빠르게 이탈하거나 상세페이지까지 도달하지 않는 트래픽과 구분될 수 있는 수치임
* **광고 게재 최적화 기준을 랜딩 페이지 조회로 꼭 선택**해야함

<br>

# 💡 Good Answer
* 내가 가진 궁금증을 바탕으로 데이터분석을 통해 내가 좋아하는 서비스의 성장에 도움이 되는 일을 하고 싶음
* 사용자분들이 우리 서비스를 더욱 만족하면서 사용할 수 있도록 만들고, 이를 통해서 비즈니스가 성장하고 다시 사용자 만족도가 높아지는 선순환을 만드는 일
  * 데이터를 기반으로 어떻게 서비스를 성장시킬 수 있을지에 대한 고민
  * 인사이트를 도출해서 유관부서에 공유하고 실행
  * 실제로 서비스가 잘 성장할 수 있도록 돕기
* '데이터분석'이라는 무기를 통해 해결해야하는 문제를 발견하고 풀어내기
* 동기부여 : 분석 업무가 실제로 서비스 목표 달성에 도움이 되었다는 것을 확인 (운영 관점에서 개선)
* 커뮤니케이션 : 원하는 바를 데이터 조건과 상황에 맞춰 구체화하고, 분석 내용을 이해시키는 과정을 더 잘할 수 있을까 고민
  * 분석 결과 공유 뿐 아니라, 분석하기 전 서비스와 운영 관점에서 가진 문제가 어떤 것인지를 논의하는 시작부터 소통이 필요
* 제한된 리소스 내에서 다양하고 고도화되는 니즈를 잘 충족시키고 최선의 방향성을 찾아가기 위한 고민 多
* 방대한 데이터를 가지고 가설을 세우고 검증하면서, 비즈니스 성장에 직접적으로 도움이 되는 결과물을 만들어내는 과정
* 도메인 이해도
  * 같은 데이터를 보더라도 그 안에서 숨겨진 맥락과 원인을 찾고 어떤 인사이트를 도출하는지에 따라서 분석 내용이 달라짐
  * 서비스와 사용자에 대한 애정과 관심을 바탕으로 도메인 이해도가 높아야 함
* 꾸준함
  * 실패와 과정을 두려워하지 않고 끊임없이 고민하면서 적극적으로 답을 찾으려는 성향 중요
* 문제 해결 능력
  * 스스로 문제 의식을 가지고 실제 데이터를 활용해 해결책을 찾고, 개선하기 위해 실행해본 경험
  * 실제로 어떤 문제를 놓고 왜 그것이 문제라고 생각했고 어떻게 문제를 해결하고 활용했는지

<br>

# 💡 궁금한 부분
* 비활성 유저들은 로그 데이터가 적어서 모델리잉 어렵고, 콘텐츠 수명이 짧은 편임. 서비스 구현 방식에 따라 다르지만 이럴때는 피드백 속도와 딜레마가 있을 것으로 예상됨. 이건 어떻게 해결하고 있는지?
  

<br>

### Reference
* [데이터 직무 면접 질문 모음집 (feat. 모범 답변, 합격 꿀팁까지)](https://zero-base.co.kr/event/media_insight_contents_DS_da_interview)
* [데이터 관련 직무 면접 대비용 개념 한 줄 정리](https://stellarway.tistory.com/8)
* [갈아먹는 통계 기초[4] 가설, 검정, p-value](https://yeomko.tistory.com/m/37?category=879901)
* [중심극한정리(CLT; central limit theorem)](https://biology-statistics-programming.tistory.com/72)
* [Handling imbalanced datasets](http://dmqm.korea.ac.kr/activity/seminar/343)
* [Conv1D, Conv2D, Conv3D 차이](https://leeejihyun.tistory.com/37)
* [CNN - Kernel & Feature map](https://woochan-autobiography.tistory.com/876)
* Complex Pattern Jacquard Fabrics Defect Detection Using Convolutional Neural Networks and Multispectral Imaging (2022)
* Automatic Ship Detection Based on RetinaNet Using Multi-Resolution Gaofen-3 Imagery (2019)
* [당신의 A/B Test를 신뢰할 수 없는 이유](https://be-favorite.github.io/Presentation_archive/Datayanolja_2023_Trustworthy_ABtest.pdf)
* [신뢰구간을 이용한 A/B Test](https://datarian.io/blog/overlapping-ci-in-abtest)
* [베이지안 A/B 테스트 in Python](https://playinpap.github.io/bayesian-abtest/)
* [분석 목적에 맞는 고객 세그먼테이션 방법을 찾아서](https://playinpap.github.io/customer-segmentation/)
* [LSTM-AE를 이용한 시퀀스 데이터 이상 탐지](https://pasus.tistory.com/267)
